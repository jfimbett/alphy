File: app/dashboard\page.tsx
----------------------------------------
'use client';

import { useRef, useState, useEffect } from 'react';
import { useRouter } from 'next/navigation';

// Components
import Navbar from '@/components/Navbar';
import FileTree, { FileNode } from '@/components/FileTree';
import SelectedFilePanel from '@/components/dashboard/FilePreviewSection';
import ChatSection from '@/components/dashboard/ChatSection';
import ChatContextRadioButtons from '@/components/dashboard/RadioButtons';
import ModelSelector from '@/components/dashboard/ModelSelector';
import FileAnalysisButtons from '@/components/dashboard/FileAnalysisProgress';
import SaveModal from '@/components/dashboard/SaveSessionModal';
import LoadModal from '@/components/dashboard/LoadSessionModal';
import FileUploadArea from '@/components/dashboard/FileUploadArea'; // We'll use this now


// Hooks
import { useFileProcessing } from './useFileProcessing';
import { useChat } from './useChat';

// Utility
import { addBase64ToTree, convertTree } from './utils/fileTreeHelpers';

// Icons (optional usage)
import { InformationCircleIcon } from '@heroicons/react/24/outline';

// Types
import { SessionSummary } from '@/app/history/page';

type ExistingUpload = {
  upload_id: number;
  upload_name: string;
};

export default function Dashboard() {
  const router = useRouter();
  const formRef = useRef<HTMLFormElement | null>(null);

  // ---------------------------
  // State for File Processing
  // ---------------------------
  const {
    fileTree, setFileTree, extractedTexts, setExtractedTexts, summaries, setSummaries, extractedCompanies, 
    rawResponses,
    isAnalyzing, processingPhase, progress, totalFiles, 
    processedFiles, processZip, processFolder, analyzeFiles, 
    toggleAllFiles, saveHeavyData,
  } = useFileProcessing();

  // ---------------------------
  // State for Chat
  // ---------------------------
  const { contextType, setContextType, chatMessage, setChatMessage, chatHistory, setChatHistory, isChatLoading, handleChatSubmit,
  } = useChat();

  // ---------------------------
  // Local UI States
  // ---------------------------
  const [selectedModel, setSelectedModel] = useState('deepseek:deepseek-reasoner');
  const [currentZipName, setCurrentZipName] = useState<string | null>(null);
  const [highlightedFiles, setHighlightedFiles] = useState<Set<string>>(new Set());
  const [showExtracted, setShowExtracted] = useState(false);
  const [allSelected, setAllSelected] = useState(true);
  const [currentSessionId, setCurrentSessionId] = useState<string | null>(null);
  const [selectedFile, setSelectedFile] = useState<FileNode | null>(null);
  const [successMessage, setSuccessMessage] = useState('');

  // ---------------------------
  // Save Modal States
  // ---------------------------
  const [showSaveModal, setShowSaveModal] = useState(false);
  const [existingUploads, setExistingUploads] = useState<ExistingUpload[]>([]);
  const [selectedUploadOption, setSelectedUploadOption] = useState<'new' | 'existing'>('new');
  const [newUploadName, setNewUploadName] = useState('');
  const [existingUploadId, setExistingUploadId] = useState<number | null>(null);
  const [fetchingUploads, setFetchingUploads] = useState(false);

  // ---------------------------
  // Load Modal States
  // ---------------------------
  const [showLoadModal, setShowLoadModal] = useState(false);
  const [availableSessions, setAvailableSessions] = useState<SessionSummary[]>([]);

  // Print to console the variables that might be used later
  console.log(
    fetchingUploads,
    existingUploadId,
    selectedUploadOption,
    existingUploads,
    currentSessionId,
    currentZipName
  );

  // ---------------------------
  // On Mount: Check User Auth
  // ---------------------------
  useEffect(() => {
    const userId = typeof window !== 'undefined' ? localStorage.getItem('userId') : null;
    if (!userId) {
      router.push('/login');
    }
  }, [router]);

  // ---------------------------
  // Helper to get all files
  // ---------------------------
  const getAllFiles = (nodes: FileNode[]): FileNode[] => {
    return nodes.flatMap((node) => {
      if (node.type === 'folder' && node.children) {
        return getAllFiles(node.children);
      }
      return node.type === 'file' ? [node] : [];
    });
  };

  // ---------------------------
  // File Selection
  // ---------------------------
  const handleFileSelect = (node: FileNode) => {
    if (node.type === 'folder') return;
    if (!node.content) node.content = ''; // Ensure content is always a string
    setSelectedFile(node);
  };

  // ---------------------------
  // "Save Session" Modal
  // ---------------------------
  const openSaveModal = async () => {
    setNewUploadName('');
    setExistingUploadId(null);
    setSelectedUploadOption('new');
    try {
      setFetchingUploads(true);
      const res = await fetch('/api/uploads', {
        headers: { 'x-user-id': localStorage.getItem('userId') || '' },
      });
      if (!res.ok) throw new Error('Failed to fetch existing uploads');
      const data = await res.json();
      setExistingUploads(data.uploads || []);
    } catch (err) {
      console.error('Error fetching uploads:', err);
      setExistingUploads([]);
    } finally {
      setFetchingUploads(false);
      setShowSaveModal(true);
    }
  };

  const closeSaveModal = () => {
    setShowSaveModal(false);
  };

  const handleSaveConfirm = async () => {
    try {
      await saveSession();
      setShowSaveModal(false);
    } catch (error) {
      console.error('Error in handleSaveConfirm:', error);
      alert('Error saving data: ' + (error as Error).message);
    }
  };

  async function saveSession(): Promise<string> {
    // 1) Convert rawData -> base64
    const fileTreeWithBase64 = addBase64ToTree(fileTree);

    // 2) Create a minimal session row in DB
    const res = await fetch('/api/sessions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-user-id': localStorage.getItem('userId') || '',
      },
      body: JSON.stringify({
        sessionName: newUploadName.trim(),
      }),
    });
    if (!res.ok) throw new Error('Failed to save session');
    const data = await res.json();
    setCurrentSessionId(data.session_id);

    // 3) Store heavy data (fileTree, chatHistory, extracted, summaries)
    await saveHeavyData(data.session_id, {
      fileTree: fileTreeWithBase64,
      extractedTexts,
      summaries,
      extractedCompanies,
    });

    setSuccessMessage('Session saved successfully!');
    setTimeout(() => setSuccessMessage(''), 5000);

    return data.session_id;
  }

  // ---------------------------
  // "Load Session" Modal
  // ---------------------------
  const handleLoadClick = async () => {
    try {
      const res = await fetch('/api/sessions', {
        headers: { 'x-user-id': localStorage.getItem('userId') || '' },
      });
      const data = await res.json();
      setAvailableSessions(data.sessions);
      setShowLoadModal(true);
    } catch {
      alert('Error loading sessions');
    }
  };

  const confirmLoadSession = async (sessionId: string) => {
    try {
      const userId = localStorage.getItem('userId');
      if (!userId) {
        router.push('/login');
        return;
      }

      // 1) Confirm the session is available
      const response = await fetch('/api/sessions', {
        headers: { 'x-user-id': userId },
      });
      if (!response.ok) throw new Error('Failed to load session');
      const data = await response.json();
      if (!data.sessions || data.sessions.length === 0) {
        alert('No session data found.');
        return;
      }

      setCurrentSessionId(sessionId);

      // 2) Fetch heavy data
      const heavyRes = await fetch(`/api/store-heavy-data?sessionId=${sessionId}`);
      if (!heavyRes.ok) throw new Error('Failed to load heavy data');
      const heavyData = await heavyRes.json();

      // 3) Rebuild the fileTree from base64
      const rebuiltTree = convertTree(heavyData.fileTree || [], sessionId);
      setFileTree(rebuiltTree);

      // 4) Restore chat history, extracted texts, summaries
      setChatHistory(heavyData.chatHistory || []);
      setExtractedTexts(heavyData.extractedTexts || {});
      setSummaries(heavyData.summaries || {});
    } catch (error) {
      console.error('Error loading session:', error);
      alert('Error loading session: ' + (error as Error).message);
    }
  };

  // ---------------------------
  // Rendering
  // ---------------------------
  return (
    <div className="min-h-screen bg-gray-50 relative">
      <Navbar />
      <main className="max-w-7xl mx-auto px-4 py-8">

        {/* Success Message */}
        {successMessage && (
          <div className="mb-4 bg-green-100 border border-green-200 text-green-800 p-3 rounded-md">
            {successMessage}
          </div>
        )}

        {/* -----
             File Upload Area
             You can remove the inline code if your <FileUploadArea> handles everything.
         ----- */}
        <div className="bg-white rounded-lg shadow-sm p-6 mb-6">
          <FileUploadArea
            processZip={processZip}
            processFolder={processFolder}
            handleLoadClick={handleLoadClick}
            isDragActive={false}
          />
        </div>

        {/* Model Selector + Optional Info Icon */}
        <div className="flex items-center gap-2 mb-4">
          <ModelSelector selectedModel={selectedModel} onModelChange={setSelectedModel} />
          {/* Optionally show an info icon */}
          <InformationCircleIcon className="h-5 w-5 text-gray-500" title="Select your model" />
        </div>

        {/* If we have a file tree, show Analyze + Save Buttons */}
        {fileTree.length > 0 && (
          <div className="bg-white rounded-lg shadow-sm p-6 mb-6">
            <div className="mb-4 flex items-center justify-between">
              <FileAnalysisButtons
                fileTree={fileTree}
                selectedModel={selectedModel}
                analyzeFiles={analyzeFiles}
                openSaveModal={openSaveModal}
                toggleAllFiles={toggleAllFiles}
                allSelected={allSelected}
                setAllSelected={setAllSelected}
                getAllFiles={getAllFiles}
                isAnalyzing={isAnalyzing}
                progress={progress}
                processingPhase={processingPhase}
                processedFiles={processedFiles}
                totalFiles={totalFiles}
              />
            </div>

            {/* Actual File Tree */}
            <FileTree
              nodes={fileTree}
              onSelect={handleFileSelect}
              selectedFile={selectedFile ? { ...selectedFile, content: selectedFile.content || '' } : null}
              onToggleConversion={(path) => {
                const updateNodes = (nodes: FileNode[]): FileNode[] =>
                  nodes.map((n) => ({
                    ...n,
                    selected: n.fullPath === path ? !n.selected : n.selected,
                    children: n.children ? updateNodes(n.children) : undefined,
                  }));
                setFileTree((prev) => updateNodes(prev));
              }}
              onToggleHighlight={(path) => {
                const newHighlighted = new Set(highlightedFiles);
                if (newHighlighted.has(path)) {
                  newHighlighted.delete(path);
                } else {
                  newHighlighted.add(path);
                }
                setHighlightedFiles(newHighlighted);

                const updateNodes = (nodes: FileNode[]): FileNode[] =>
                  nodes.map((n) => ({
                    ...n,
                    highlighted: newHighlighted.has(n.fullPath!),
                    children: n.children ? updateNodes(n.children) : undefined,
                  }));
                setFileTree(updateNodes(fileTree));
              }}
            />
          </div>
        )}

        {/* Selected File Preview */}
        {selectedFile && (
          <SelectedFilePanel
            selectedFile={{ ...selectedFile, content: selectedFile.content || '' }}
            extractedTexts={extractedTexts}
            extractedCompanies={extractedCompanies}
            summaries={summaries}
            showExtracted={showExtracted}
            setShowExtracted={setShowExtracted}
            rawResponses={rawResponses}
          />
        )}

        {/* Radio Buttons to change "contextType"? */}
        <ChatContextRadioButtons fileTree={fileTree} getAllFiles={getAllFiles} />

        {/* Chat Section */}
        <ChatSection
          chatHistory={chatHistory}
          formRef={formRef}
          handleChatSubmit={handleChatSubmit}
          extractedTexts={extractedTexts}
          selectedFile={selectedFile}
          highlightedFiles={highlightedFiles}
          chatMessage={chatMessage}
          setChatMessage={setChatMessage}
          isChatLoading={isChatLoading}
          selectedModel={selectedModel}
        />

        {/* Save Modal */}
        <SaveModal
          showSaveModal={showSaveModal}
          newUploadName={newUploadName}
          setNewUploadName={setNewUploadName}
          closeSaveModal={closeSaveModal}
          handleSaveConfirm={handleSaveConfirm}
        />

      </main>

      {/* Load Modal (rendered outside main for simplicity) */}
      <LoadModal
        showLoadModal={showLoadModal}
        availableSessions={availableSessions}
        confirmLoadSession={confirmLoadSession}
        setShowLoadModal={setShowLoadModal}
      />
    </div>
  );
}

----------------------------------------

File: app/dashboard\useChat.tsx
----------------------------------------
'use client';
import { useState } from 'react';

interface ChatMessage {
  role: string;
  content: string;
}

export type ContextType = 'none' | 'local' | 'global';

export function useChat() {
  const [contextType, setContextType] = useState<ContextType>('none');
  const [chatMessage, setChatMessage] = useState('');
  const [chatHistory, setChatHistory] = useState<ChatMessage[]>([]);
  const [isChatLoading, setIsChatLoading] = useState(false);

  // Submit chat to your LLM
  const handleChatSubmit = async ({
    e,
    selectedFileText,
    globalContext,
    model
  }: {
    e: React.FormEvent<HTMLFormElement>;
    selectedFileText: string;
    globalContext: string;
    model: string;
  }) => {
    e.preventDefault();
    if (!chatMessage.trim()) return;
  
    setIsChatLoading(true);
    const newHistory = [...chatHistory, { role: 'user', content: chatMessage }];

    try {
      let context = '';
      if (contextType === 'local' && selectedFileText) {
        context = selectedFileText;
      } else if (contextType === 'global' && globalContext) {
        // Example: limit the length if needed
        context = globalContext.slice(0, 5000);
      }

      const res = await fetch('/api/llm', {
        method: 'POST',
        headers: { 
          'Content-Type': 'application/json' ,
          'x-user-id': localStorage.getItem('userId') || ''
        },
        body: JSON.stringify({ 
          prompt: chatMessage,
          context: context,
          history: newHistory,
          model: model,
          format: contextType === 'global' ? 'json' : undefined
         }),
      });

  
      const data = await res.json();
      if (!res.ok) throw new Error(data.error || 'Chat failed');
  
      setChatHistory([...newHistory, { role: 'assistant', content: data.content }]);
      setChatMessage('');
      
    } catch (error) {
      console.error('Chat Error:', error);
      setChatHistory(prev => [...prev, {
        role: 'assistant',
        content: `Error: ${(error as Error).message} - Please try again`
      }]);
    } finally {
      setIsChatLoading(false);
    }
  };

  return {
    contextType,
    setContextType,
    chatMessage,
    setChatMessage,
    chatHistory,
    setChatHistory,
    isChatLoading,
    handleChatSubmit,
  };
}

----------------------------------------

File: app/dashboard\useFileProcessing.tsx
----------------------------------------
'use client';
import { useState } from 'react';
import JSZip from 'jszip';
import { getDocument, GlobalWorkerOptions } from 'pdfjs-dist';
import * as XLSX from 'xlsx';
import { TextItem } from 'pdfjs-dist/types/src/display/api';
import { FileNode } from '@/components/FileTree';
// Add to existing imports
import { CompanyInfo } from '@/app/types';
import { requestToBodyStream } from 'next/dist/server/body-streams';

// IMPORTANT: pdf.js worker config
GlobalWorkerOptions.workerSrc = '/pdf.worker.mjs';

const DEVELOPMENT = process.env.NEXT_PUBLIC_LLM_DEV_MODE === 'development';

interface FilePayload {
  path: string;
  base64Data: string; // We'll store file data as base64 from the start
  blobUrl: string;
}

// Helper to convert ArrayBuffer to base64
function arrayBufferToBase64(buffer: ArrayBuffer): string {
  let binary = '';
  const bytes = new Uint8Array(buffer);
  for (let i = 0; i < bytes.length; i++) {
    binary += String.fromCharCode(bytes[i]);
  }
  return btoa(binary);
}

// Helper to decode base64 back to ArrayBuffer
function base64ToArrayBuffer(base64: string): ArrayBuffer {
  const binary = atob(base64);
  const len = binary.length;
  const buffer = new ArrayBuffer(len);
  const bytes = new Uint8Array(buffer);
  for (let i = 0; i < len; i++) {
    bytes[i] = binary.charCodeAt(i);
  }
  return buffer;
}

export function useFileProcessing() {

  
  const [rawResponses, setRawResponses] = useState<Record<string, string>>({});
  const [extractedCompanies, setExtractedCompanies] = useState<Record<string, CompanyInfo[]>>({});
  const [fileTree, setFileTree] = useState<FileNode[]>([]);
  const [extractedTexts, setExtractedTexts] = useState<Record<string, string>>({});
  const [summaries, setSummaries] = useState<Record<string, string>>({});

  // For progress indicators
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [processingPhase, setProcessingPhase] = useState<'extracting' | 'summarizing' | 'idle' | 'extracting_companies'>('idle');
  const [progress, setProgress] = useState(0);
  const [totalFiles, setTotalFiles] = useState(0);
  const [processedFiles, setProcessedFiles] = useState(0);

  // ======================
  // Build File Tree
  // ======================
  const buildFileTree = (files: FilePayload[]): FileNode[] => {
    const root: FileNode = { name: '', type: 'folder', children: [] };

    files.forEach(({ path, base64Data, blobUrl }) => {
      const parts = path.split('/');
      let current = root;
      const pathSegments: string[] = [];

      parts.forEach((part, i) => {
        if (!part) return;
        pathSegments.push(part);

        const existing = current.children?.find((n) => n.name === part);
        if (existing) {
          current = existing;
        } else {
          const isFile = i === parts.length - 1;
          const newNode: FileNode = {
            name: part,
            type: isFile ? 'file' : 'folder',
            children: isFile ? undefined : [],
            // Store base64 in node.base64Data
            base64Data: isFile ? base64Data : undefined,
            // content for preview
            content: isFile ? blobUrl : undefined,
            // We no longer store rawData to avoid detachment
            fullPath: pathSegments.join('/'),
          };
          if (!current.children) current.children = [];
          current.children.push(newNode);
          current = newNode;

          // Default to selected
          if (isFile) {
            newNode.selected = true;
          }
        }
      });
    });

    return root.children || [];
  };

  // ======================
  // ZIP UPLOAD PROCESSING
  // ======================
  const processZip = async (file: File) => {
    const zip = new JSZip();
    const zipContent = await zip.loadAsync(file);

    const files = await Promise.all(
      Object.values(zipContent.files)
        .filter((entry) => !entry.dir)
        .map(async (entry) => {
          // 1) Get raw arraybuffer
          const data = await entry.async('arraybuffer');
          // 2) Immediately convert to base64
          const base64Data = arrayBufferToBase64(data);
          // 3) For browser preview
          const blobUrl = URL.createObjectURL(new Blob([data]));
          return {
            path: entry.name,
            base64Data,
            blobUrl,
          };
        })
    );

    setFileTree(buildFileTree(files));
  };

  // ======================
  // FOLDER UPLOAD
  // ======================
  const processFolder = async (fileList: FileList) => {
    const filePromises = Array.from(fileList).map((file) => {
      return new Promise<FilePayload>((resolve, reject) => {
        const reader = new FileReader();
        reader.onload = () => {
          if (reader.result && typeof reader.result !== 'string') {
            // Convert arraybuffer to base64
            const base64Data = arrayBufferToBase64(reader.result);
            const blobUrl = URL.createObjectURL(file);
            resolve({
              path: file.webkitRelativePath,
              base64Data,
              blobUrl,
            });
          } else {
            reject(new Error('Failed to read file as ArrayBuffer'));
          }
        };
        reader.onerror = (err) => reject(err);
        reader.readAsArrayBuffer(file);
      });
    });

    try {
      const files = await Promise.all(filePromises);
      setFileTree(buildFileTree(files));
    } catch (err) {
      console.error('Error reading folder files:', err);
    }
  };

  // Helper: get all files in a tree
  const getAllFiles = (nodes: FileNode[]): FileNode[] => {
    return nodes.flatMap((node) => {
      if (node.type === 'folder' && node.children) {
        return getAllFiles(node.children);
      }
      return node.type === 'file' ? [node] : [];
    });
  };

  // ======================
  // ANALYZE FILES
  // ======================
  const analyzeFiles = async (model: string) => {
    try {
      const allFiles = getAllFiles(fileTree).filter((f) => f.selected);

      // Phase 1: Extract text
      setProcessingPhase('extracting');
      setIsAnalyzing(true);
      setProgress(0);
      setProcessedFiles(0);

      const total = allFiles.length;
      setTotalFiles(total);

      const newExtractedTexts: Record<string, string> = {};
      let processedCount = 0;

      for (const node of allFiles) {
        let extracted = '';

        // If for some reason it's missing base64Data, skip
        if (!node.base64Data) continue;

        // Decode to ArrayBuffer for PDF or XLSX
        const arrayBuffer = base64ToArrayBuffer(node.base64Data);

        // PDF Extraction
        if (node.name.toLowerCase().endsWith('.pdf')) {
          try {
            const data = new Uint8Array(arrayBuffer);
            const pdf = await getDocument({ data }).promise;
            let text = '';
            for (let i = 1; i <= pdf.numPages; i++) {
              const page = await pdf.getPage(i);
              const content = await page.getTextContent();
              text += content.items
                .filter((item): item is TextItem => 'str' in item)
                .map((item: TextItem) => item.str)
                .join(' ') + '\n';
            }
            extracted = text;
          } catch (err) {
            console.error(`Failed to extract text from ${node.name}`, err);
            extracted = '[Error extracting PDF text]';
          }
        }
        // Excel Extraction
        else if (node.name.toLowerCase().match(/\.(xlsx|xls)$/)) {
          try {
            const workbook = XLSX.read(new Uint8Array(arrayBuffer), { type: 'array' });
            let excelText = '';
            workbook.SheetNames.forEach((sheetName) => {
              const worksheet = workbook.Sheets[sheetName];
              const sheetAsJson = XLSX.utils.sheet_to_json(worksheet, { header: 1 });
              (sheetAsJson as (string | number | boolean | null)[][]).forEach((row) => {
                excelText += row.join(' ') + '\n';
              });
              excelText += '\n';
            });
            extracted = excelText;
          } catch (err) {
            console.error(`Failed to extract text from ${node.name}`, err);
            extracted = '[Error extracting Excel text]';
          }
        }
        // Other file types
        else {
          extracted = '[Text extraction not available for this file type]';
        }

        newExtractedTexts[node.fullPath!] = extracted.trim().replace(/\s+/g, ' ');

        processedCount++;
        setProcessedFiles(processedCount);
        setProgress(Math.round((processedCount / total) * 100));
      }

     


      setExtractedTexts(newExtractedTexts);

      // Phase 2: Summarization
      setProcessingPhase('summarizing');
      setProgress(0);
      setProcessedFiles(0);

      const newSummaries: Record<string, string> = {};
      let summaryCount = 0;
      const textEntries = Object.entries(newExtractedTexts);

      for (const [fullPath, text] of textEntries) {
        try {
          const prompt = `You are a Summarization Assistant. Your job is to read the text below—written in any language—and produce a single-paragraph summary in clear, fluent English. Focus on the following:

              Key financial metrics (e.g., revenue, assets, profitability)
              Risks (e.g., market risks, operational risks)
              Opportunities (e.g., potential growth, strategic advantages)

          Instructions:

              Write exactly one paragraph.
              Emphasize the most relevant financial details, along with notable risks and opportunities.
              Avoid minor or irrelevant information.
              Output only the summarized text, with no extra commentary, headings, or disclaimers.

          Document Text: ${text}`;

          if (DEVELOPMENT) {
            newSummaries[fullPath] = 'Some random text for development purposes...';
          } else {
            const res = await fetch('/api/llm', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                prompt,
                history: [],
                model: model,
                requestType: 'summarize',
              }),
            });

            if (!res.ok) {
              console.error('Summary API error:', res.statusText);
              newSummaries[fullPath] = 'Summary failed: API error';
            } else {
              const data = await res.json();
              newSummaries[fullPath] = data.content;
            }
          }
        } catch (error) {
          newSummaries[fullPath] = `Summary failed: ${(error as Error).message}`;
        }

        summaryCount++;
        setProcessedFiles(summaryCount);
        setProgress(Math.round((summaryCount / textEntries.length) * 100));
      }

      setSummaries(newSummaries);


      setProcessingPhase('extracting_companies');
      setProgress(0);
      setProcessedFiles(0);

      // We extract first the variables that are present

      let variables  = '';

      for (const [fullPath, text] of textEntries) {
        const variables_prompt = `
        Out of the following text, identify what financial varables are referenced, the text can be written in languages different than english, return me only the list of variables without the values

        ["var1", "var2", ...]

        here is the text, return the name of the variables in english, also be consistent across names and try not to duplicate them, Im looking mostly for accounting variables
        Also return the names of the variables in lower case and with underscores instead of spaces.


        ${text}
        `

        const res = await fetch('/api/llm', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            prompt: variables_prompt,
            model: model,
            format: 'json',
            requestType: 'extract',
          }),
        });

        // append the variables to the variables string
        if (res.ok) {
          const data = await res.json();
          const contentWithFence = data.content || '';
          const cleaned = contentWithFence
            .replace(/```json\s*/i, '')
            .replace(/```/g, '')
            .trim();

          variables += cleaned;
        }
      }


      const newExtractedCompanies: Record<string, CompanyInfo[]> = {};
      let companyCount = 0;
      
      for (const [fullPath, text] of textEntries) {
        try {
          // Simpler extraction prompt, no repeated quotes
          const extractionPrompt = `
      You are an Information Extraction Assistant. 
      Your task is to read the given text (which may appear in any language) and extract 
      any company-level financial data into a well-structured JSON array. 
      There could be many be multiple companies mentioned, so please generate an array entry for each 
      distinct company. These are the potential fields you may encounter (not necessarily all will be present, nor they will be in english)
      
      Variables:
      ${variables}


Rules:

    Output ONLY valid JSON: Do not include markdown, explanations, or any text outside the JSON.
    Use EXACT values from the text (including any currency symbols) for numeric fields.
    Skip any fields that are not present in the text (do not output null or empty strings).
    If multiple statements for different years are found, list all those years in the "years" array and include the corresponding values in their respective objects (e.g., "profits", "assets", etc.).
    If multiple companies are mentioned in the text, create a separate object for each company.
    Do not include any additional text or explanation outside of the JSON array.

Example Output (This is just an illustrative example, not tied to any specific document): [ { "name": "Example Corporation Inc.", "org_number": "987654321", "sector": "Retail", "years": ["2021", "2020"], "profits": { "2021": "USD 1,000,000", "2020": "USD 950,000" }, "assets": { "2021": "USD 5,000,000", "2020": "USD 4,500,000" }, "revenue": { "2021": "USD 3,000,000", "2020": "USD 2,700,000" } } ]

Document Text:\n\n ${text}`;
      
          if (DEVELOPMENT) {
            newExtractedCompanies[fullPath] = [{
              name: 'Example Corp',
              sector: 'Technology',
              profits: { '2022': 1500000, '2023': 2000000 },
              assets: { '2022': 5000000, '2023': 6000000 },
              years: [2022, 2023]
            }];
          } else {
            const res = await fetch('/api/llm', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                prompt: extractionPrompt,
                model: model,
                format: 'json',
                requestType: 'extract',
              }),
            });
      
            if (res.ok) {
              const data = await res.json();
              // data.content might have triple backticks, so remove them:
              const contentWithFence = data.content || '';
              const cleaned = contentWithFence
                .replace(/```json\s*/i, '')
                .replace(/```/g, '')
                .trim();
      
                const rawResponse = data.content;
                setRawResponses(prev => ({...prev, [fullPath]: rawResponse}));

              try {
                // Expecting an array
                const companies: CompanyInfo[] = JSON.parse(cleaned);
                newExtractedCompanies[fullPath] = companies;
              } catch (e) {
                console.error('Failed to parse company data:', e);
                newExtractedCompanies[fullPath] = [];
              }
            } else {
              newExtractedCompanies[fullPath] = [];
            }
          }
        } catch (error) {
          newExtractedCompanies[fullPath] = [];
        }
      
        companyCount++;
        setProcessedFiles(companyCount);
        setProgress(Math.round((companyCount / textEntries.length) * 100));
      }
      
      setExtractedCompanies(newExtractedCompanies);
      
      } catch (error) {
        console.error('Processing error:', error);
      } finally {
        setIsAnalyzing(false);
        setProcessingPhase('idle');
      }
    };

  // ======================
  // SELECT/DESELECT ALL
  // ======================
  const toggleAllFiles = (selected: boolean) => {
    const updateNodes = (nodes: FileNode[]): FileNode[] =>
      nodes.map((n) => ({
        ...n,
        selected: n.type === 'file' ? selected : n.selected,
        children: n.children ? updateNodes(n.children) : undefined,
      }));
    setFileTree((prev) => updateNodes(prev));
  };

   // Example snippet after analysis completes OR on "Save Session":
   const saveHeavyData = async (
    sessionId: string,
    heavyData: {
      fileTree: FileNode[];
      extractedTexts: Record<string, string>;
      summaries: Record<string, string>;
      extractedCompanies: Record<string, CompanyInfo[]>;
    }
  ) => {
    try {
      const res = await fetch('/api/store-heavy-data', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          sessionId,
          heavyData,
        }),
      });
      if (!res.ok) throw new Error('Failed to save heavy data');
     
    } catch (error) {
      console.error(error);
      throw error;
    }
  };

  return { fileTree, setFileTree, extractedTexts, setExtractedTexts, summaries, setSummaries, 
    isAnalyzing, processingPhase, progress, totalFiles, processedFiles, processZip, 
    processFolder, analyzeFiles, toggleAllFiles, buildFileTree, saveHeavyData, 
    extractedCompanies, setExtractedCompanies, rawResponses, setRawResponses
  };
}

----------------------------------------

File: app/dashboard\utils\fileTreeHelpers.tsx
----------------------------------------
// app\dashboard\utils\fileTreeHelpers.ts
import { FileNode } from '@/components/FileTree';

export function addBase64ToTree(nodes: FileNode[]): FileNode[] {
  return nodes.map((node) => {
    if (node.type === 'folder' && node.children) {
      return { ...node, children: addBase64ToTree(node.children) };
    }
    if (node.type === 'file' && node.rawData) {
      const uint8 = new Uint8Array(node.rawData);
      let binary = '';
      for (let i = 0; i < uint8.length; i++) {
        binary += String.fromCharCode(uint8[i]);
      }
      const base64Data = btoa(binary);
      return { ...node, base64Data };
    }
    return node;
  });
}

export function convertTree(nodes: FileNode[], sessionId: number | string): FileNode[] {
  return nodes.map((node) => {
    if (node.type === 'folder' && node.children) {
      return { ...node, children: convertTree(node.children, sessionId) };
    }
    if (node.type === 'file') {
      // Convert base64 => rawData if present
      if (node.base64Data) {
        const binaryString = atob(node.base64Data);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        node.rawData = bytes.buffer;
        node.base64Data = undefined; // free it up
      }
      // Build the inline content path if localPath is present
      if (node.localPath) {
        node.content = `/api/session-file?sessionId=${sessionId}&filePath=${encodeURIComponent(
          node.localPath || ''
        )}`;
      }
    }
    return node;
  });
}

----------------------------------------

File: app/history\page.tsx
----------------------------------------
'use client';

import { useEffect, useState } from 'react';
import { useRouter } from 'next/navigation';
import { TrashIcon } from '@heroicons/react/24/outline';
import Navbar from '@/components/Navbar';

export interface SessionSummary {
  session_id: number;
  session_name: string;
  created_at: string;
  file_count: number;
}

export default function HistoryPage() {
  const router = useRouter();
  const [sessions, setSessions] = useState<SessionSummary[]>([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState('');

  // Fetch user sessions on mount
  useEffect(() => {
    const userId = localStorage.getItem('userId');
    if (!userId) {
      // Not logged in
      router.push('/login');
      return;
    }

    fetch('/api/sessions', {
      headers: { 'x-user-id': userId }
    })
      .then((res) => res.json())
      .then((data) => {
        if (data.error) {
          setError(data.error);
          return;
        }
        setSessions(data.sessions || []);
      })
      .catch((err) => {
        setError(err.message || 'Error fetching sessions');
      })
      .finally(() => {
        setLoading(false);
      });
  }, [router]);

  // Delete a session
  async function handleDeleteSession(sessionId: number) {
    const userId = localStorage.getItem('userId') || '';
     // First confirmation:
    if (!confirm('Are you sure you want to delete this session?')) return;
    // Second confirmation:
    if (!confirm('This action is permanent and cannot be undone. Continue?')) return;


    try {
      const res = await fetch(`/api/sessions/${sessionId}`, {
        method: 'DELETE',
        headers: { 'x-user-id': userId }
      });
      if (!res.ok) {
        const d = await res.json();
        throw new Error(d.error || 'Failed to delete session');
      }

      // Remove it from local state
      setSessions((prev) => prev.filter((s) => s.session_id !== sessionId));
    } catch (err: unknown) {
      if (err instanceof Error) {
        alert('Error deleting session: ' + err.message);
      } else {
        alert('Error deleting session');
      }
      console.error(err);
    }
  }

  if (loading) {
    return <div className="p-4">Loading sessions...</div>;
  }

  if (error) {
    return <div className="p-4 text-red-600">Error: {error}</div>;
  }

  return (
    <div className="min-h-screen bg-gray-50">
      <Navbar />
      <main className="max-w-4xl mx-auto px-4 py-8">
        <h1 className="text-2xl font-bold mb-6 text-gray-800">Your Sessions</h1>
        {sessions.length === 0 ? (
          <p>No sessions found.</p>
        ) : (
          <ul className="space-y-4">
            {sessions.map((session) => (
              <li key={session.session_id} className="border p-4 rounded bg-white flex justify-between">
                <div>
                  <h2 className="text-lg font-semibold text-gray-600">
                    {session.session_name}
                  </h2>
                  <p className="text-sm text-gray-600">
                    Created: {new Date(session.created_at).toLocaleString()}
                  </p>
                  <p className="text-sm text-gray-600">
                    {/*Files: {session.file_count}*/}
                  </p>
                </div>
                <div className="flex items-center gap-2">
                  {/* View Details */}
                  <button
                    onClick={() => router.push(`/history/${session.session_id}`)}
                    className="bg-blue-600 text-white px-3 py-1 rounded hover:bg-blue-700"
                  >
                    View Details
                  </button>

                  {/* Trash Icon => Delete Session */}
                  <button
                    onClick={() => handleDeleteSession(session.session_id)}
                    className="p-2 hover:bg-red-50 rounded"
                    title="Delete Session"
                  >
                    <TrashIcon className="w-5 h-5 text-red-600" />
                  </button>
                </div>
              </li>
            ))}
          </ul>
        )}
      </main>
    </div>
  );
}

----------------------------------------

File: app/history\[sessionId]\page.tsx
----------------------------------------
// app/history/[sessionId]/page.tsx
'use client';

import { useEffect, useState } from 'react';
import { useParams } from 'next/navigation';
import Navbar from '@/components/Navbar';
import Footer from '@/components/Footer';
import ReactJson from 'react-json-view';
import * as XLSX from 'xlsx';
import { FileNode } from '@/components/FileTree';

// Define the structure of the session data stored in the database.
export interface SessionData {
  session_name: string;
  fileTree?: FileNode[];
  // You can add other properties (extractedTexts, summaries, chatHistory, etc.) here
  [key: string]: unknown;
}

// Define the session structure returned by your API.
export interface Session {
  session_data: SessionData;
  created_at: string;
  file_count: number;
}

// A helper function to recursively extract Excel files from the file tree.
const getExcelFiles = (nodes: FileNode[]): FileNode[] => {
  let files: FileNode[] = [];
  nodes.forEach((node) => {
    if (node.type === 'file' && /\.(xlsx|xls)$/i.test(node.name)) {
      files.push(node);
    }
    if (node.children && node.children.length > 0) {
      files = files.concat(getExcelFiles(node.children));
    }
  });
  return files;
};

// Component to preview an Excel file node.
const ExcelPreview: React.FC<{ node: FileNode }> = ({ node }) => {
  const [tableData, setTableData] = useState<string[][]>([]);
  const [error, setError] = useState<string>('');

  useEffect(() => {
    const parseExcel = async () => {
      if (!node.base64Data) {
        setError('No data available');
        return;
      }
      try {
        // Decode the base64 string into a binary string.
        const binaryStr = atob(node.base64Data);
        const len = binaryStr.length;
        const bytes = new Uint8Array(len);
        for (let i = 0; i < len; i++) {
          bytes[i] = binaryStr.charCodeAt(i);
        }
        // Parse the workbook using XLSX.
        const workbook = XLSX.read(bytes, { type: 'array' });
        const sheetName = workbook.SheetNames[0];
        const sheet = workbook.Sheets[sheetName];
        // Convert the sheet to an array-of-arrays (each row is an array).
        const data: (string | number | boolean | null)[][] = XLSX.utils.sheet_to_json<(string | number | boolean | null)[]>(sheet, { header: 1 });
        const stringData: string[][] = data.map(row => row.map(cell => cell !== null ? cell.toString() : ''));
        setTableData(stringData);
      } catch {
        setError('Error parsing Excel file');
      }
    };

    parseExcel();
  }, [node.base64Data]);

  return (
    <div className="mb-4">
      <h3 className="font-semibold text-gray-800">{node.name}</h3>
      {error ? (
        <p className="text-red-600">{error}</p>
      ) : tableData.length > 0 ? (
        <div className="overflow-x-auto">
          <table className="min-w-full border border-gray-300">
            <tbody>
              {tableData.slice(0, 10).map((row, rowIndex) => (
                <tr key={rowIndex} className="border-b border-gray-200">
                  {row.map((cell, cellIndex) => (
                    <td
                      key={cellIndex}
                      className="px-2 py-1 border-r border-gray-200 text-gray-800"
                    >
                      {cell !== undefined ? cell.toString() : ''}
                    </td>
                  ))}
                </tr>
              ))}
            </tbody>
          </table>
          {tableData.length > 10 && (
            <p className="text-sm text-gray-600 mt-2">Showing first 10 rows</p>
          )}
        </div>
      ) : (
        <p className="text-gray-800">No data to preview</p>
      )}
    </div>
  );
};

const SessionDetail: React.FC = () => {
  const { sessionId } = useParams();
  const [session, setSession] = useState<Session | null>(null);
  const [error, setError] = useState<string>('');

// Add this useEffect to fetch heavy data when the component mounts
useEffect(() => {

  const userId = localStorage.getItem('userId');
  if (!userId || !sessionId) return;

  const fetchHeavyData = async () => {
    try {
      const res = await fetch(`/api/store-heavy-data?sessionId=${sessionId}`);
      if (!res.ok) throw new Error('Failed to fetch heavy data');
      const heavyData = await res.json();
      // Merge heavy data into session data
      setSession(prev => prev ? {
        ...prev,
        session_data: {
          ...prev.session_data,
          extractedTexts: heavyData.extractedTexts,
          summaries: heavyData.summaries
        }
      } : null);
    } catch (err) {
      if (err instanceof Error) {
        setError(err.message);
      } else {
        setError('An unknown error occurred');
      }
    }
  };

  async function fetchSession() {
    try {
      // 1) Minimal session info from the DB
      const sessRes = await fetch(`/api/sessions/${sessionId}`, {
        headers: { 'x-user-id': userId || '' },
      });
      if (!sessRes.ok) throw new Error('Failed to fetch session');
      const sessJson = await sessRes.json();
      // sessJson is { session: { session_id, session_name, created_at, ... } }

      // 2) Heavy data from local file
      const heavyRes = await fetch(`/api/store-heavy-data?sessionId=${sessionId}`);
      // If heavy data doesn’t exist, it might be 404. You can handle that gracefully.
      if (!heavyRes.ok) {
        console.warn('No heavy data found (maybe not uploaded yet).');
      }
      const heavyJson = heavyRes.ok ? await heavyRes.json() : {};

      // 3) Merge them together
      // Suppose your DB session JSON doesn't store `session_data` by default.
      // You can create a structure that includes fileTree, chatHistory, etc.
      setSession({
        // For example, you can store them like:
        session_data: {
          session_name: sessJson.session.session_name,
          fileTree: heavyJson.extractedTexts ? [] : [], // or reconstruct if you wish
          // Or you can do more advanced merges if you have them
          extractedTexts: heavyJson.extractedTexts || {},
          summaries: heavyJson.summaries || {},
        },
        created_at: sessJson.session.created_at,
        file_count: sessJson.session.file_count || 0,
      });
    } catch (err) {
      console.error('Error fetching session details:', err);
      setError(err instanceof Error ? err.message : 'Unknown error');
    }
  }

  if (sessionId) {
    fetchSession();
    fetchHeavyData();
  }
}, [sessionId]);

  if (error) {
    return (
      <div className="min-h-screen bg-gray-50 flex flex-col">
        <Navbar />
        <main className="max-w-7xl mx-auto px-4 py-8 flex-grow text-gray-800">
          <p className="text-red-600">{error}</p>
        </main>
        <Footer />
      </div>
    );
  }

  if (!session) {
    return (
      <div className="min-h-screen bg-gray-50 flex flex-col">
        <Navbar />
        <main className="max-w-7xl mx-auto px-4 py-8 flex-grow text-gray-800">
          <p>Loading session details...</p>
        </main>
        <Footer />
      </div>
    );
  }

  // Extract Excel files (if any) from the file tree stored in session_data.
  const excelFiles: FileNode[] = session.session_data.fileTree
    ? getExcelFiles(session.session_data.fileTree)
    : [];

  return (
    <div className="min-h-screen bg-gray-50 flex flex-col">
      <Navbar />
      <main className="max-w-7xl mx-auto px-4 py-8 flex-grow text-gray-800">
        <h1 className="text-2xl font-bold mb-6">Session Details</h1>
        <div className="mb-4">
          <p>
            <strong>Name:</strong>{' '}
            {session.session_data.session_name || 'Untitled Session'}
          </p>
          <p>
            <strong>Created:</strong>{' '}
            {new Date(session.created_at).toLocaleString()}
          </p>
          <p>
           {/* <strong>Files Count:</strong> {session.file_count}*/}
          </p>
        </div>

        <div className="mb-6">
          <h2 className="text-xl font-semibold mb-2">Session Data</h2>
          {/* A scrollable, collapsible JSON viewer */}
          <div className="max-h-96 overflow-y-auto bg-gray-100 p-4 rounded">
            <ReactJson
              src={session.session_data}
              theme="rjv-default"
              collapsed={2}
              displayDataTypes={false}
              style={{ fontSize: '0.875rem' }}
            />
          </div>
        </div>

        {excelFiles.length > 0 && (
          <div className="mb-6">
            <h2 className="text-xl font-semibold mb-2">Excel Files Preview</h2>
            {excelFiles.map((file, index) => (
              <ExcelPreview key={index} node={file} />
            ))}
          </div>
        )}
      </main>
      <Footer />
    </div>
  );
};

export default SessionDetail;

----------------------------------------

File: app/api\account-data\route.tsx
----------------------------------------
// app/api/account-data/route.ts
import { NextResponse } from "next/server"

const BASE_URL = process.env.NEXT_PUBLIC_EXTERNAL_API_BASE_URL
const API_TOKEN = process.env.NEXT_PUBLIC_EXTERNAL_API_TOKEN

export async function GET(request: Request) {
  try {
    const { searchParams } = new URL(request.url)
    // e.g. "AccountsPayableCurrent"
    const accountParam = searchParams.get("account") ?? ""
    // e.g. "2022" or "2023"
    const yearParam = searchParams.get("year") ?? ""

    if (!accountParam) {
      return NextResponse.json({ error: "Missing 'account' query param" }, { status: 400 })
    }

    // Remote endpoint might be something like:
    //    /all_data_for_account?account=AccountsPayableCurrent&year=2022&api_token=...
    const remoteUrl = `${BASE_URL}/all_data_for_account?account=${accountParam}&year=${yearParam}&api_token=${API_TOKEN}`

    const response = await fetch(remoteUrl)
    if (!response.ok) {
      return NextResponse.json(
        { error: `Upstream error: ${response.statusText}` },
        { status: response.status }
      )
    }

    const data = await response.json()
    return NextResponse.json(data)
  } catch (err) {
    console.error("Error in /api/account-data route:", err)
    return NextResponse.json({ error: "Internal server error" }, { status: 500 })
  }
}

----------------------------------------

File: app/api\all-accounts\route.tsx
----------------------------------------
// app/api/all-accounts/route.ts
import { NextResponse } from "next/server"

const BASE_URL = process.env.NEXT_PUBLIC_EXTERNAL_API_BASE_URL 
const API_TOKEN = process.env.NEXT_PUBLIC_EXTERNAL_API_TOKEN  

export async function GET() {
  try {
    // Build remote URL
    const remoteUrl = `${BASE_URL}/all_accounts?api_token=${API_TOKEN}`
    console.log("Fetching from:", remoteUrl) 
    // Server-to-server fetch => no CORS issue
    const response = await fetch(remoteUrl)
    if (!response.ok) {
      return NextResponse.json(
        { error: `Upstream error: ${response.statusText}` },
        { status: response.status }
      )
    }

    const data = await response.json()
    return NextResponse.json(data)
  } catch (err) {
    console.error("Error in /api/all-accounts route:", err)
    return NextResponse.json({ error: "Internal server error" }, { status: 500 })
  }
}

----------------------------------------

File: app/api\auth\login\route.tsx
----------------------------------------
'use server';
import { NextResponse } from 'next/server';
import pool from '@/utils/db';
import bcrypt from 'bcryptjs';

export async function POST(request: Request) {
  try {
    const body = await request.json();
    const { email, password } = body;
    if (!email || !password) {
      return NextResponse.json({ error: 'Email and password are required' }, { status: 400 });
    }
    const client = await pool.connect();
    const userRes = await client.query('SELECT * FROM users WHERE email = $1', [email]);
    client.release();
    if (userRes.rowCount === 0) {
      return NextResponse.json({ error: 'Invalid credentials' }, { status: 401 });
    }
    const user = userRes.rows[0];
    const passwordMatch = bcrypt.compareSync(password, user.password_hash);
    if (!passwordMatch) {
      return NextResponse.json({ error: 'Invalid credentials' }, { status: 401 });
    }
    // Return user data (do not include the password hash)
    const userData = { user_id: user.user_id, email: user.email };
    return NextResponse.json({ success: true, user: userData }, { status: 200 });
  } catch (error) {
    console.error('Error in login:', error);
    return NextResponse.json({ error: 'Error logging in' }, { status: 500 });
  }
}

----------------------------------------

File: app/api\auth\signup\route.tsx
----------------------------------------
'use server';
import { NextResponse } from 'next/server';
import pool from '@/utils/db';
import bcrypt from 'bcryptjs';

export async function POST(request: Request) {
  try {
    const body = await request.json();
    const { email, password, company, reason } = body;
    if (!email || !password) {
      return NextResponse.json({ error: 'Email and password are required' }, { status: 400 });
    }
    const client = await pool.connect();
    // Check if the user already exists
    const userCheck = await client.query('SELECT * FROM users WHERE email = $1', [email]);
    if (userCheck && userCheck.rowCount !== null && userCheck.rowCount > 0) {
      client.release();
      return NextResponse.json({ error: 'User already exists' }, { status: 400 });
    }
    // Hash the password
    const salt = bcrypt.genSaltSync(10);
    const password_hash = bcrypt.hashSync(password, salt);
    // Insert new user
    const result = await client.query(
      `INSERT INTO users (email, password_hash, company, reason) 
       VALUES ($1, $2, $3, $4) RETURNING user_id, email`,
      [email, password_hash, company, reason]
    );
    client.release();
    return NextResponse.json({ success: true, user: result.rows[0] }, { status: 200 });
  } catch (error) {
    console.error('Error in signup:', error);
    return NextResponse.json({ error: 'Error signing up' }, { status: 500 });
  }
}

----------------------------------------

File: app/api\company-facts\route.tsx
----------------------------------------
// app/api/company-facts/route.ts

import { NextResponse } from "next/server"

// We'll assume you're using .env for the base URL and token.
// If not, you can hardcode them here.
const BASE_URL = process.env.NEXT_PUBLIC_EXTERNAL_API_BASE_URL
const API_TOKEN = process.env.NEXT_PUBLIC_EXTERNAL_API_TOKEN

export async function GET(request: Request) {
  try {
    // 1) Extract query params from the incoming request
    const { searchParams } = new URL(request.url)
    // e.g. "320193"
    const cik = searchParams.get("cik") ?? ""

    // 2) Build the remote URL
    const remoteUrl = `${BASE_URL}/company_facts?cik=${cik}&api_token=${API_TOKEN}`

    console.log("Fetching from:", remoteUrl)

    // 3) Fetch from the external API (server-to-server, no CORS issues here)
    const response = await fetch(remoteUrl, {
      method: "GET",
    })

    if (!response.ok) {
      // e.g. 404, 500, ...
      return NextResponse.json(
        { error: `Upstream error: ${response.statusText}` },
        { status: response.status }
      )
    }

    // 4) Return the JSON response to the client
    const data = await response.json()
    return NextResponse.json(data)
  } catch (err) {
    return NextResponse.json({ error: "Internal server error" }, { status: 500 })
  }
}

----------------------------------------

File: app/api\files\route.tsx
----------------------------------------
import { NextResponse } from 'next/server';
import pool from '@/utils/db';

export async function POST(request: Request) {
  const client = await pool.connect();
  try {
    const userId = request.headers.get('x-user-id');
    if (!userId) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }

    const formData = await request.formData();
    const files = formData.getAll('files') as File[];
    
    if (!files || files.length === 0) {
      return NextResponse.json({ error: 'No files provided' }, { status: 400 });
    }

    const insertedFiles = [];
    
    for (const file of files) {
      const buffer = Buffer.from(await file.arrayBuffer());
      const result = await client.query(
        `INSERT INTO files 
         (user_id, file_name, file_type, file_data, session_id)
         VALUES ($1, $2, $3, $4, $5)
         RETURNING file_id`,
         [userId, file.name, file.type, buffer, request.headers.get('x-session-id')]
      );
      insertedFiles.push(result.rows[0].file_id);
    }

    return NextResponse.json({ success: true, fileIds: insertedFiles });
  } catch (error) {
    console.error('File upload error:', error);
    return NextResponse.json({ error: 'File upload failed' }, { status: 500 });
  } finally {
    client.release();
  }
}

export async function GET() {
  try {
    const client = await pool.connect();
    const result = await client.query(`SELECT * FROM files;`);
    client.release();

    return NextResponse.json(result.rows);
  } catch (error) {
    console.error(error);
    return NextResponse.json({ error: 'Error fetching files' }, { status: 500 });
  }
}

----------------------------------------

File: app/api\files\[fileId]\route.tsx
----------------------------------------
import { NextResponse } from 'next/server';
import pool from '@/utils/db';


export async function GET(
  request: Request,
  { params }: { params: { fileId: string } }
) {
  const client = await pool.connect();
  try {
    const fileId = parseInt(params.fileId);
    if (isNaN(fileId)) {
      return NextResponse.json({ error: 'Invalid file ID' }, { status: 400 });
    }

    const result = await client.query(
      `SELECT file_name, file_type, file_data 
       FROM files 
       WHERE file_id = $1`,
      [fileId]
    );

    if (result.rowCount === 0) {
      return NextResponse.json({ error: 'File not found' }, { status: 404 });
    }

    const file = result.rows[0];
    return new NextResponse(file.file_data, {
      headers: {
        'Content-Type': file.file_type,
        'Content-Disposition': `attachment; filename="${file.file_name}"`
      }
    });
  } catch (error) {
    console.error('File download error:', error);
    return NextResponse.json({ error: 'File download failed' }, { status: 500 });
  } finally {
    client.release();
  }
}

export async function DELETE(
  request: Request,
  { params }: { params: { fileId: string } }
) {
  let client;
  try {
    client = await pool.connect();
    const fileId = parseInt(params.fileId, 10);
    if (isNaN(fileId)) {
      return NextResponse.json({ error: 'Invalid file ID' }, { status: 400 });
    }

    // Remove any extractions for this file, then remove the file itself
    await client.query('DELETE FROM extractions WHERE file_id = $1', [fileId]);
    await client.query('DELETE FROM files WHERE file_id = $1', [fileId]);

    client.release();
    return NextResponse.json({ success: true }, { status: 200 });
  } catch (error) {
    console.error('Error deleting file:', error);
    if (client) client.release();
    return NextResponse.json({ error: 'Error deleting file' }, { status: 500 });
  }
}

----------------------------------------

File: app/api\financial-data\route.tsx
----------------------------------------
// app/api/financial-data/route.ts
import { NextResponse } from "next/server"
import { Pool } from 'pg';

const pool = new Pool({
    port: 5432,
    host: 'localhost',
    user: 'postgres',
    password: '1234',
    database: 'sec_financial_data',
});

export async function GET(request: Request) {
  try {
    // 1) Parse query params from the URL
    const { searchParams } = new URL(request.url)
    const nameParam = (searchParams.get("name") ?? "").trim().toLowerCase()
    const tickerParam = (searchParams.get("ticker") ?? "").trim().toLowerCase()
    const cikParam = (searchParams.get("cik") ?? "").trim()

    // 2) Prepare placeholders.
    //    For name, we use a LIKE pattern if it's not empty: '%<name>%'
    //    For ticker, we do an exact match but case-insensitive, so we store it in lowercase and compare with LOWER(t.ticker).
    //    For cik, we do an exact match.
    const nameFilter = nameParam ? `%${nameParam}%` : ""
    const tickerFilter = tickerParam // empty if none provided
    const cikFilter = cikParam // empty if none provided

    // 3) Connect to the database
    const client = await pool.connect()

    // 4) Build and run the query
    //    Explanation:
    //    - LEFT JOIN `tickers` so we can gather possible tickers per `companies` row
    //    - Filter with a combination of name, ticker, and/or cik
    //    - If the corresponding parameter is empty, we skip that filter
    //    - Group by c.cik, c.name so we can do array_agg on the tickers
    const sql = `
      SELECT
        c.cik,
        c.name,
        ARRAY_AGG(t.ticker) AS tickers
      FROM companies c
      LEFT JOIN tickers t ON c.cik = t.cik
      WHERE
        ($1 = '' OR LOWER(c.name) LIKE $1)
        AND ($2 = '' OR LOWER(t.ticker) = $2)
        AND ($3 = '' OR c.cik = $3)
      GROUP BY c.cik, c.name
      ORDER BY c.name ASC
      LIMIT 50;
    `

    const result = await client.query(sql, [
      nameFilter,
      tickerFilter,
      cikFilter,
    ])

    client.release()

    // 5) Return JSON response
    return NextResponse.json(result.rows)
  } catch (error) {
    console.error("Error in /api/financial-data:", error)
    return NextResponse.json(
      { error: "Internal server error" },
      { status: 500 }
    )
  }
}

----------------------------------------

File: app/api\llm\route.tsx
----------------------------------------
// app/api/llm/route.ts
import { NextResponse } from 'next/server'
import pool from '@/utils/db';

export const dynamic = 'force-dynamic';

interface DeepSeekError {
  error: {
    message: string;
    type: string;
    code: string;
  };
}

export async function POST(req: Request) {
  const body = await req.json();
  const { prompt, context, history, model, format, requestType } = body || {};

  if (!prompt || typeof prompt !== 'string') {
    return NextResponse.json({ error: "Invalid prompt" }, { status: 300 });
  }

  const DEVELOPMENT = process.env.NEXT_PUBLIC_LLM_DEV_MODE === 'true';
  const userId = req.headers.get('x-user-id');

  if (DEVELOPMENT) {
   
    await new Promise(res => setTimeout(res, 500));
    return NextResponse.json({
      content: `[MOCK RESPONSE] ${model || 'no-model'} response...`,
      tokensUsed: 42
    });
  }

  try {
    
    if (model?.startsWith('deepseek:')) {
      const deepseekKey = process.env.DEEPSEEK_API_KEY;
      if (!deepseekKey) {
        console.error('DeepSeek API key missing');
        return NextResponse.json(
          { error: 'DeepSeek API key not configured' },
          { status: 500 }
        );
      }

      const modelName = model.replace('deepseek:', '');
      const messages = [
        { role: 'system', content: context || 'You are a helpful assistant.' },
        ...(Array.isArray(history) ? history : []),
        { role: 'user', content: prompt }
      ];

      const requestPayload = {
        model: modelName,
        messages,
        temperature: 0.1,
        max_tokens: 8000,
        stream: false,
        //response_format: format === 'json' ? { type: 'json_object' } : undefined
      };

      const startTime = Date.now();
      const response = await fetch('https://api.deepseek.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${deepseekKey}`
        },
        body: JSON.stringify(requestPayload)
      });

      const responseTime = Date.now() - startTime;
      const responseText = await response.text();
      
      

      if (!response.ok) {
        console.error('DeepSeek API Error Details:', {
          status: response.status,
          headers: Object.fromEntries(response.headers.entries()),
          body: responseText
        });
        return NextResponse.json(
          { error: `DeepSeek API Error: ${response.statusText}`, details: responseText },
          { status: 500 }
        );
      }

      try {
        const data = JSON.parse(responseText);
        return NextResponse.json({
          content: data.choices[0].message.content,
          tokensUsed: data.usage?.total_tokens || 0
        });
      } catch (parseError) {
        console.error('Response JSON Parse Error:', parseError);
        return NextResponse.json(
          { error: 'Failed to parse API response', response: responseText },
          { status: 500 }
        );
      }
    }

    if (model?.startsWith('openai:')) {
      // commented for now
    }

    return NextResponse.json(
      { error: 'Unsupported model provider' },
      { status: 400 }
    );

  } catch (error) {
    console.error('LLM Processing Error:', error);
    return NextResponse.json(
      { error: 'LLM processing failed', details: error instanceof Error ? error.message : String(error) },
      { status: 500 }
    );
  }
}
----------------------------------------

File: app/api\search\route.tsx
----------------------------------------
// app/api/search/route.ts
import { NextResponse } from 'next/server';
import pool from '../../../utils/db';

export async function GET(request: Request) {
  const { searchParams } = new URL(request.url);
  const query = searchParams.get('query') || '';

  try {
    const client = await pool.connect();
    const searchQuery = `
      SELECT 
        c.cik,
        c.name,
        ARRAY_AGG(t.ticker) as tickers,
        ts_rank(to_tsvector('english', c.name), plainto_tsquery('english', $1)) as rank
      FROM companies c
      LEFT JOIN tickers t ON c.cik = t.cik
      WHERE 
        to_tsvector('english', c.name) @@ plainto_tsquery('english', $1) OR
        t.ticker = $1 OR
        c.cik = $1
      GROUP BY c.cik, c.name
      ORDER BY rank DESC
      LIMIT 10;
    `;

    const result = await client.query(searchQuery, [query]);
    client.release();

    return NextResponse.json(result.rows.map(row => ({
      ...row,
      searchTerm: row.tickers.includes(query.toUpperCase()) ? query.toUpperCase() : row.cik === query ? query : row.name
    })));
  } catch  {
    return NextResponse.json(
      { error: 'Internal server error' },
      { status: 500 }
    );
  }
}
----------------------------------------

File: app/api\session-file\route.tsx
----------------------------------------
import { NextResponse } from 'next/server';
import fs from 'fs';
import path from 'path';
import mime from 'mime'; // install via `npm install mime` if you want to do more robust MIME detection

export async function GET(request: Request) {
  try {
    const { searchParams } = new URL(request.url);
    const sessionId = searchParams.get('sessionId');
    const filePath = searchParams.get('filePath');

    if (!sessionId || !filePath) {
      return NextResponse.json(
        { error: 'Missing sessionId or filePath' },
        { status: 400 }
      );
    }

    // Construct the absolute path on disk
    const absolutePath = path.join(
      process.cwd(),
      'data',
      sessionId,
      decodeURIComponent(filePath) // Add URI decoding
    );

    console.log('Serving file:', absolutePath);

    if (!fs.existsSync(absolutePath)) {
      return NextResponse.json({ error: 'File not found' }, { status: 404 });
    }

    // Read file from disk
    const fileBuffer = fs.readFileSync(absolutePath);

    // Infer MIME type from the filename extension, or fall back to octet-stream
    const mimeType = mime.getType(absolutePath) || 'application/octet-stream';

    const headers = {
      'Content-Type': mimeType,
      'Content-Disposition': `inline; filename="${path.basename(absolutePath)}"`,
    };
    // Add proper PDF content disposition
    if (mimeType === 'application/pdf') {
      headers['Content-Disposition'] = `inline; filename="${path.basename(absolutePath)}"`;
    } else {
      headers['Content-Disposition'] = `attachment; filename="${path.basename(absolutePath)}"`;
    }

    // Return the file bytes in the response
    return new NextResponse(fileBuffer, {
      headers: headers,
    });
  } catch (error) {
    console.error('Error serving file:', error);
    return NextResponse.json({ error: 'File serving error' }, { status: 500 });
  }
}

----------------------------------------

File: app/api\sessions\route.tsx
----------------------------------------
// app/api/sessions/route.tsx
import { NextResponse } from 'next/server';
import pool from '../../../utils/db';
// app/api/sessions/route.tsx
export async function GET(request: Request) {
  const client = await pool.connect();
  try {
    const userId = request.headers.get('x-user-id');
    if (!userId || isNaN(parseInt(userId))) { // Add numeric validation
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }
  

    const result = await client.query(`
      SELECT 
        s.session_id,
        s.session_name,
        s.created_at,
        (SELECT COUNT(*)::int FROM files f WHERE f.session_id = s.session_id) AS file_count
      FROM sessions s
      WHERE s.user_id = $1
      ORDER BY s.created_at DESC
    `, [userId]);


    
    const sessions = result.rows.map((row) => ({
      session_id: row.session_id,
      session_name: row.session_name,
      created_at: row.created_at,
      file_count: row.file_count
    }));

    return NextResponse.json({ sessions });
  } catch (error) {
    console.error('Session load error:', error);
    //alert(error);
    return NextResponse.json({ error: 'Failed to load sessions' }, { status: 500 });
  } finally {
    client.release();
  }
}



export async function POST(request: Request) {
  const client = await pool.connect();
  await client.query('BEGIN');
  try {
    const userId = request.headers.get('x-user-id');
    if (!userId) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }

    const { sessionName } = await request.json();


    //columns  session_id | user_id | session_name | created_at | expires_at
    const result = await client.query(`
      INSERT INTO sessions (user_id, session_name)
      VALUES ($1, $2)
      RETURNING session_id, created_at
    `, [userId, sessionName]);

   
    const sessionId = result.rows[0].session_id;

    await client.query('COMMIT');
    return NextResponse.json({
      success: true,
      session_id: sessionId,
      created_at: result.rows[0].created_at
    });
  } catch (error) {
    await client.query('ROLLBACK');
    console.error('Session save error:', error);
    return NextResponse.json({ error: 'Failed to save session' }, { status: 500 });
  } finally {
    client.release();
  }
}
----------------------------------------

File: app/api\sessions\[sessionId]\route.tsx
----------------------------------------
// app/api/sessions/[sessionId]/route.tsx
import { NextResponse } from 'next/server';
import pool from '@/utils/db';
import fs from 'fs';
import path from 'path';

export async function GET(
  request: Request,
  { params }: { params: { sessionId: string } }
) {
  const client = await pool.connect();
  try {
    const userId = request.headers.get('x-user-id');
    if (!userId) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }
    const sId = parseInt(params.sessionId, 10);
    if (isNaN(sId)) {
      return NextResponse.json({ error: 'Invalid session ID' }, { status: 400 });
    }

    const res = await client.query(
      `SELECT session_id, session_name, created_at
         FROM sessions
        WHERE session_id = $1 AND user_id = $2`,
      [sId, userId]
    );
    if (res.rowCount === 0) {
      return NextResponse.json({ error: 'Session not found' }, { status: 404 });
    }
    const session = res.rows[0];
    return NextResponse.json({ session });
  } catch (error) {
    console.error('Error fetching session:', error);
    return NextResponse.json({ error: 'Error fetching session' }, { status: 500 });
  } finally {
    client.release();
  }
}

export async function DELETE(
  request: Request,
  { params }: { params: { sessionId: string } }
) {
  const client = await pool.connect();
  await client.query('BEGIN');
  try {
    const userId = request.headers.get('x-user-id');
    if (!userId) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }
    const sId = parseInt(params.sessionId, 10);
    if (isNaN(sId)) {
      return NextResponse.json({ error: 'Invalid session ID' }, { status: 400 });
    }
    // Optionally, remove the session reference from files (set session_id to NULL)
    await client.query(
      `UPDATE files SET session_id = NULL WHERE session_id = $1 AND user_id = $2`,
      [sId, userId]
    );
    const deleteResult = await client.query(
      `DELETE FROM sessions WHERE session_id = $1 AND user_id = $2`,
      [sId, userId]
    );
    if (deleteResult.rowCount === 0) {
      // Nothing to delete or unauthorized
      await client.query('ROLLBACK');
      return NextResponse.json(
        { error: 'Session not found or not authorized' },
        { status: 404 }
      );
    }

    await client.query('COMMIT');

     // 3) Remove the session folder from disk (best-effort)
    // data/<sessionId> is the folder holding heavyData.json + /files
    const sessionPath = path.join(process.cwd(), 'data', String(sId));
    try {
      // Node 14 and below do not support fs.rmSync, so if you need older Node, use rmdirSync.
      fs.rmSync(sessionPath, { recursive: true, force: true });
      console.log(`Deleted directory: ${sessionPath}`);
    } catch (err) {
      // Not critical if folder removal fails – but we log it.
      console.error(`Failed to remove folder ${sessionPath}:`, err);
    }

    return NextResponse.json({ success: true });
  } catch (error) {
    await client.query('ROLLBACK');
    console.error('Error deleting session:', error);
    return NextResponse.json({ error: 'Error deleting session' }, { status: 500 });
  } finally {
    client.release();
  }
}

----------------------------------------

File: app/api\store-heavy-data\route.tsx
----------------------------------------
// app/api/store-heavy-data/route.ts
import { NextResponse } from 'next/server';
import fs from 'fs';
import path from 'path';
import { FileNode } from '@/components/FileTree';

export async function POST(request: Request) {
  try {
    const { sessionId, heavyData } = await request.json();
    if (!sessionId) {
      return NextResponse.json({ error: 'sessionId is required' }, { status: 400 });
    }

    // 1) Create the directory structure: data/sessionId/files
    const dataDir = path.join(process.cwd(), 'data', sessionId.toString());
    const filesDir = path.join(dataDir, 'files');
    if (!fs.existsSync(dataDir)) {
      fs.mkdirSync(dataDir, { recursive: true });
    }
    if (!fs.existsSync(filesDir)) {
      fs.mkdirSync(filesDir);
    }

    // 2) For each file node in fileTree, decode base64 -> raw file
    function storeFilesRecursively(nodes: FileNode[]): FileNode[] {
      return nodes.map((node, idx) => {
        if (node.type === 'folder' && node.children) {
          return { ...node, children: storeFilesRecursively(node.children) };
        }
        if (node.type === 'file' && node.base64Data) {
          // decode base64
          const buffer = Buffer.from(node.base64Data, 'base64');
          // create a unique filename. You can also keep the original name if you prefer
          // but we add idx or a timestamp to avoid collisions:
          const safeName = node.name.replace(/[^\w\d.]+/g, '_');
          const fileName = `file_${Date.now()}_${idx}_${safeName}`;
          const filePath = path.join(filesDir, fileName);

          // 3) Write the file to data/sessionId/files/
          fs.writeFileSync(filePath, buffer);

          // 4) Remove base64Data from the node, and add localPath
          return {
            ...node,
            base64Data: undefined,
            rawData: undefined,
            localPath: `files/${fileName}`
          };
        }
        return node;
      });
    }

    // If there's a fileTree, store each file on disk
    let updatedFileTree: FileNode[] = [];
    if (heavyData.fileTree) {
      updatedFileTree = storeFilesRecursively(heavyData.fileTree);
    }

    // 5) Overwrite heavyData.fileTree with the updated one
    const finalHeavyData = {
      ...heavyData,
      fileTree: updatedFileTree
    };

    // 6) Write everything to heavyData.json
    const filePath = path.join(dataDir, 'heavyData.json');
    fs.writeFileSync(filePath, JSON.stringify(finalHeavyData, null, 2));

    return NextResponse.json({ success: true, filePath });
  } catch (error) {

    return NextResponse.json(
      { error: 'Failed to save heavy data' + error },
      { status: 500 }
    );
  }
}

export async function GET(request: Request) {
  const { searchParams } = new URL(request.url);
  const sessionId = searchParams.get('sessionId');

  if (!sessionId) {
    return NextResponse.json({ error: 'sessionId is required' }, { status: 400 });
  }

  try {
    const dataDir = path.join(process.cwd(), 'data', sessionId);
    const filePath = path.join(dataDir, 'heavyData.json');
    if (!fs.existsSync(filePath)) {
      return NextResponse.json({ error: 'No heavyData found' }, { status: 404 });
    }

    const data = fs.readFileSync(filePath, 'utf-8');
    return NextResponse.json(JSON.parse(data));
  } catch (error) {
    console.error('Error reading heavy data:', error);
    return NextResponse.json({ error: 'Heavy data not found' }, { status: 404 });
  }
}

----------------------------------------

File: app/api\submission-history\route.tsx
----------------------------------------
// app/api/submission-history/route.ts

import { NextResponse } from "next/server"

const BASE_URL = process.env.NEXT_PUBLIC_EXTERNAL_API_BASE_URL
const API_TOKEN = process.env.NEXT_PUBLIC_EXTERNAL_API_TOKEN

export async function GET(request: Request) {
  try {
    const { searchParams } = new URL(request.url)
    const cik = searchParams.get("cik") ?? ""

    const remoteUrl = `${BASE_URL}/submission_history?cik=${cik}&api_token=${API_TOKEN}`
    const response = await fetch(remoteUrl, { method: "GET" })
    if (!response.ok) {
      return NextResponse.json(
        { error: `Upstream error: ${response.statusText}` },
        { status: response.status }
      )
    }

    const data = await response.json()
    return NextResponse.json(data)
  } catch (err) {
    console.error("Error in /api/submission-history route:", err)
    return NextResponse.json({ error: "Internal server error" }, { status: 500 })
  }
}

----------------------------------------

File: app/api\test\route.tsx
----------------------------------------
// app/api/test/route.ts
import { NextResponse } from 'next/server';
import pool from '@/utils/db';

export async function GET() {
  try {
    const client = await pool.connect();
    const result = await client.query('SELECT * FROM files ORDER BY file_id;');
    client.release();

    return NextResponse.json(result.rows);
  } catch (error) {
    console.error('Error fetching data', error);
    return NextResponse.json({ error: 'Error fetching data' }, { status: 500 });
  }
}

----------------------------------------

File: app/api\uploads\route.tsx
----------------------------------------
// app/api/uploads/route.ts
import { NextResponse } from 'next/server';
import pool from '@/utils/db';
import { FileNode } from '@/components/FileTree';

export async function GET(request: Request) {
  const userId = request.headers.get('x-user-id');
  if (!userId) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
  }

  try {
    const client = await pool.connect();
    // 1) Fetch all uploads for this user:
    const uploadsResult = await client.query(`
      SELECT upload_id, upload_name, created_at
      FROM uploads
      WHERE user_id = $1
      ORDER BY created_at DESC
    `, [userId]);

    const uploads = uploadsResult.rows;
    client.release();
    return NextResponse.json({ uploads });
  } catch (error) {
    console.error('Error fetching uploads:', error);
    return NextResponse.json({ error: 'Error fetching uploads' }, { status: 500 });
  }
}

export async function POST(request: Request) {
  let client;
  try {
    client = await pool.connect();
    const body = await request.json();
    const { uploadName, fileTree, extractedTexts, summaries, chatHistory } = body;

    // 1) Validate user ID from headers
    const userId = request.headers.get('x-user-id');
    if (!userId) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }

    // 2) Validate required fields
    if (!uploadName) {
      return NextResponse.json({ error: 'uploadName is required' }, { status: 400 });
    }

    // 3) Create a new "upload" row
    const insertUploadRes = await client.query(
      `INSERT INTO uploads (user_id, upload_name)
       VALUES ($1, $2)
       RETURNING upload_id`,
      [userId, uploadName]
    );
    const newUploadId = insertUploadRes.rows[0].upload_id;

    // 4) Gather all files from the fileTree
    const filesToInsert: Array<{
      fullPath: string;
      fileName: string;
      fileDataBase64?: string;
      mimeType: string;
    }> = [];

    function traverseTree(nodes: FileNode[]) {
      for (const node of nodes) {
        if (node.type === 'file') {
          filesToInsert.push({
            fullPath: node.fullPath || '',
            fileName: node.name,
            fileDataBase64: node.base64Data, // base64 string from the frontend
            mimeType: node.mimeType || 'application/octet-stream'
          });
        }
        if (node.children?.length) {
          traverseTree(node.children);
        }
      }
    }
    traverseTree(fileTree || []);

    // 5) Insert each file row + extraction if applicable
    for (const file of filesToInsert) {
      const fileDataBuffer = file.fileDataBase64
        ? Buffer.from(file.fileDataBase64, 'base64')
        : null;

      // Insert a row in "files"
      const insertFileRes = await client.query(
        `INSERT INTO files (upload_id, file_name, file_path, mime_type, file_data, is_extracted)
         VALUES ($1, $2, $3, $4, $5, $6)
         RETURNING file_id`,
        [
          newUploadId,
          file.fileName,
          file.fullPath,
          file.mimeType,
          fileDataBuffer,
          false
        ]
      );

      const fileId = insertFileRes.rows[0].file_id;

      // If there's text or summary, insert into "extractions"
      const extractedText = extractedTexts?.[file.fullPath];
      const summary = summaries?.[file.fullPath];
      if (extractedText || summary) {
        // Mark file as extracted
        await client.query(
          `UPDATE files SET is_extracted = TRUE WHERE file_id = $1`,
          [fileId]
        );

        // Insert into "extractions"
        await client.query(
          `INSERT INTO extractions (file_id, extracted_text, summarized_text)
           VALUES ($1, $2, $3)`,
          [fileId, extractedText || null, summary || null]
        );
      }
    }

    client.release();

    // Return the upload_id and chatHistory (if any)
    return NextResponse.json(
      { upload_id: newUploadId, chatHistory },
      { status: 200 }
    );
  } catch (error) {
    console.error('Error creating new upload:', error);
    if (client) client.release();
    return NextResponse.json(
      { error: 'Error creating new upload' },
      { status: 500 }
    );
  }
}

----------------------------------------

File: app/api\uploads\[uploadId]\route.tsx
----------------------------------------

import { NextResponse } from 'next/server';
import pool from '@/utils/db';
import { FileNode } from '@/components/FileTree';


export async function DELETE(request: Request,{ params }: { params: { uploadId: string } }) {
  let client;
  try {
    client = await pool.connect();
    const uploadId = parseInt(params.uploadId,10);
    if(isNaN(uploadId)) {
      return NextResponse.json({ error:'Invalid upload ID'},{ status:400 });
    }
    //1) Find all files for this upload
    const fileRes = await client.query(
      'SELECT file_id FROM files WHERE upload_id=$1',[uploadId]
    );
    const fileIds = fileRes.rows.map(r=>r.file_id);
    //2) Delete extractions for these files
    if(fileIds.length>0) {
      await client.query(
        'DELETE FROM extractions WHERE file_id=ANY($1)',
        [fileIds]
      );
    }
    //3) Delete files themselves
    await client.query('DELETE FROM files WHERE upload_id=$1',[uploadId]);
    //4) Finally delete the upload
    await client.query('DELETE FROM uploads WHERE upload_id=$1',[uploadId]);
    client.release();
    return NextResponse.json({ success:true },{ status:200 });
  } catch(error) {
    console.error('Error deleting entire upload:',error);
    if(client) client.release();
    return NextResponse.json({ error:'Error deleting upload'},{ status:500 });
  }
}


export async function PATCH(request: Request,{ params }: { params: { uploadId: string } }) {
  let client;
  try {
    client = await pool.connect();
    const uploadId = parseInt(params.uploadId, 10);
    if (isNaN(uploadId)) {
      return NextResponse.json({ error: 'Invalid upload ID' }, { status: 400 });
    }

    const body = await request.json();
    const { fileTree, extractedTexts, summaries } = body;
    const filesToInsert: Array<{
      fullPath: string;
      fileName: string;
      fileDataBase64?: string;
      mimeType: string;
    }> = [];

    function traverseTree(nodes: FileNode[]) {
      for (const node of nodes) {
        if (node.type === 'file') {
          filesToInsert.push({
            fullPath: node.fullPath || '',
            fileName: node.name,
            fileDataBase64: node.base64Data, // from front end
            mimeType: node.mimeType || 'application/octet-stream'
          });
        }
        if (node.children?.length) traverseTree(node.children);
      }
    }
    traverseTree(fileTree);

    // For each file, either insert or update
    for (const file of filesToInsert) {
      const existingFileRes = await client.query(
        `SELECT file_id FROM files WHERE upload_id = $1 AND file_path = $2`,
        [uploadId, file.fullPath]
      );
      let fileId: number;
      if (existingFileRes.rowCount && existingFileRes.rowCount > 0) {
        fileId = existingFileRes.rows[0].file_id;
        // Potentially update file_data if new base64 data is provided
        if (file.fileDataBase64) {
          const fileDataBuffer = Buffer.from(file.fileDataBase64, 'base64');
          await client.query(
            `UPDATE files
                SET file_data = $2, mime_type=$3
              WHERE file_id = $1`,
            [fileId, fileDataBuffer, file.mimeType]
          );
        }
      } else {
        // Insert new file row
        const fileDataBuffer = file.fileDataBase64
          ? Buffer.from(file.fileDataBase64, 'base64')
          : null;
        const insertRes = await client.query(
          `INSERT INTO files (upload_id, file_name, file_path, mime_type, file_data, is_extracted)
           VALUES ($1, $2, $3, $4, $5, $6)
           RETURNING file_id`,
          [
            uploadId,
            file.fileName,
            file.fullPath,
            file.mimeType,
            fileDataBuffer,
            false
          ]
        );
        fileId = insertRes.rows[0].file_id;
      }

      // Check if we have extraction for that path
      const extractedText = extractedTexts[file.fullPath];
      const summary = summaries[file.fullPath];
      if (extractedText || summary) {
        // Mark file as extracted
        await client.query(`UPDATE files SET is_extracted = TRUE WHERE file_id = $1`, [fileId]);
        // Upsert extraction
        const existingExtractRes = await client.query(
          `SELECT extraction_id FROM extractions WHERE file_id=$1`,
          [fileId]
        );
        if (existingExtractRes.rowCount && existingExtractRes.rowCount > 0) {
          await client.query(
            `UPDATE extractions
                SET extracted_text=$2,
                    summarized_text=$3,
                    updated_at=NOW()
              WHERE file_id=$1`,
            [fileId, extractedText || null, summary || null]
          );
        } else {
          await client.query(
            `INSERT INTO extractions (file_id, extracted_text, summarized_text)
             VALUES ($1,$2,$3)`,
            [fileId, extractedText || null, summary || null]
          );
        }
      }
    }

    client.release();
    return NextResponse.json({ success: true, message: 'Upload updated.' });
  } catch (error) {
    console.error('Error updating upload:', error);
    if (client) client.release();
    return NextResponse.json({ error: 'Error updating upload' }, { status: 500 });
  }
}

----------------------------------------

File: db\reset.sql
----------------------------------------
DO $$ DECLARE
    r RECORD;
BEGIN
    FOR r IN (SELECT tablename FROM pg_tables WHERE schemaname = 'public') LOOP
        EXECUTE 'DROP TABLE IF EXISTS public.' || r.tablename || ' CASCADE';
    END LOOP;
END $$;

----------------------------------------

File: db\schema.sql
----------------------------------------
-- db/schema.sql

-- 1) Users table (unchanged)
CREATE TABLE IF NOT EXISTS users (
  user_id SERIAL PRIMARY KEY,
  email VARCHAR(255) NOT NULL UNIQUE,
  password_hash VARCHAR(255) NOT NULL,
  full_name VARCHAR(255),
  company VARCHAR(255),
  reason TEXT,
  created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- sessions table (lighter)
CREATE TABLE IF NOT EXISTS sessions (
  session_id SERIAL PRIMARY KEY,
  user_id INT NOT NULL,
  session_name VARCHAR(255),
  created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  expires_at TIMESTAMPTZ NOT NULL DEFAULT NOW() + INTERVAL '7 days',
  FOREIGN KEY (user_id) REFERENCES users(user_id)
);


CREATE TABLE IF NOT EXISTS files (
  file_id SERIAL PRIMARY KEY,
  user_id INT NOT NULL,
  session_id INT NOT NULL,
  file_name TEXT NOT NULL,
  file_path TEXT NOT NULL,
  file_size BIGINT,
  uploaded_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (user_id) REFERENCES users(user_id),
  FOREIGN KEY (session_id) REFERENCES sessions(session_id)
);


-- 4) Extractions table (modified)
CREATE TABLE IF NOT EXISTS extractions (
  extraction_id SERIAL PRIMARY KEY,
  file_id INT NOT NULL,
  extracted_text TEXT,      -- Store path to text file if large
  summarized_text TEXT,     -- Store path to summary file if large
  created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (file_id) REFERENCES files(file_id)
);

-- 5) Named Entities table (unchanged)
CREATE TABLE IF NOT EXISTS named_entities (
  entity_id SERIAL PRIMARY KEY,
  file_id INT NOT NULL,
  entity_type VARCHAR(50),
  entity_name VARCHAR(255),
  confidence NUMERIC(5, 2),
  created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (file_id) REFERENCES files(file_id)
);

-- Example: Create an uploads table
CREATE TABLE IF NOT EXISTS uploads (
  upload_id SERIAL PRIMARY KEY,
  user_id INT NOT NULL,                -- references 'users' table
  upload_name VARCHAR(255) NOT NULL,   -- name of the upload (e.g. "Financial Docs")
  upload_path TEXT,                    -- optional path or label, if needed
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (user_id) REFERENCES users(user_id)
);

-- Create api_keys table
CREATE TABLE api_keys (
    id SERIAL PRIMARY KEY,
    user_id UUID REFERENCES auth.users NOT NULL,
    provider TEXT NOT NULL,
    decrypted_key TEXT NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(user_id, provider)
);
----------------------------------------

File: db\sec_financial_data.sql
----------------------------------------
-- Create database
CREATE DATABASE sec_financial_data;
\c sec_financial_data;

-- Create tables
CREATE TABLE companies (
    cik VARCHAR(10) PRIMARY KEY,
    name VARCHAR(255) NOT NULL
);

CREATE TABLE tickers (
    id SERIAL PRIMARY KEY,
    cik VARCHAR(10) REFERENCES companies(cik),
    ticker VARCHAR(10) NOT NULL
);

CREATE TABLE financial_data (
    id SERIAL PRIMARY KEY,
    cik VARCHAR(10) REFERENCES companies(cik),
    accn VARCHAR(255) NOT NULL,
    period_start DATE NOT NULL,
    period_end DATE NOT NULL,
    location VARCHAR(50),
    value NUMERIC(15,2) NOT NULL,
    frame VARCHAR(20) NOT NULL,
    taxonomy VARCHAR(20) NOT NULL,
    units VARCHAR(10) NOT NULL,
    account_name VARCHAR(255) NOT NULL
);

-- Indexes for faster search
CREATE INDEX idx_companies_name ON companies USING gin(to_tsvector('english', name));
CREATE INDEX idx_tickers_ticker ON tickers(ticker);
CREATE INDEX idx_financial_data_cik ON financial_data(cik);

ALTER TABLE tickers
ADD CONSTRAINT unique_cik_ticker UNIQUE (cik, ticker);
----------------------------------------

File: db\testdata.sql
----------------------------------------
-- db/testdata.sql
-- 
-- This file inserts random data into the tables: users, uploads, files, extractions, named_entities.
-- 
-- Make sure you have already run your schema (db/schema.sql) so these tables exist.

-- 1) Insert some users
INSERT INTO users (email, password_hash, full_name)
VALUES 
  ('john@example.com', 'fakehash123', 'John Doe'),
  ('jane@example.com', 'fakehashABC', 'Jane Smith'),
  ('bob@example.com',  'fakehashXYZ', 'Bob Martin');

-- 2) Insert some uploads
-- Let's assume each user did a couple of uploads
-- We'll store references to user_id = 1, 2, 3 from above. 
-- The "upload_path" might be a local or S3 path, just an example string
INSERT INTO uploads (user_id, upload_name, upload_path)
VALUES
  (1, 'Q1 Financials', '/uploads/2023_04_01_Q1_financials.zip'),
  (1, 'M&A Documents', '/uploads/MnA_Acquisition_Jan.zip'),
  (2, 'Random Folder', '/uploads/RandomDataFolder.zip'),
  (3, 'Company Deck', '/uploads/CompanyDeckInfo.zip');

-- 3) Insert some files 
-- The foreign key "upload_id" must match the "upload_id" from the uploads above.
-- We'll add random PDF/Excel names to show variety
INSERT INTO files (upload_id, file_name, file_path, mime_type, is_extracted)
VALUES
  -- For the first upload (upload_id=1)
  (1, 'Fund_ABC_Financials.pdf', '/files/Fund_ABC_Financials.pdf', 'application/pdf', FALSE),
  (1, 'Q1_Statements.xls',        '/files/Q1_Statements.xls',       'application/vnd.ms-excel', FALSE),
  (1, 'Company_Overview.pdf',     '/files/Company_Overview.pdf',    'application/pdf', FALSE),

  -- For the second upload (upload_id=2)
  (2, 'M_and_A_Terms.pdf',        '/files/M_and_A_Terms.pdf',       'application/pdf', FALSE),
  (2, 'Deal_Appendix.xlsx',       '/files/Deal_Appendix.xlsx',      'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', FALSE),

  -- For the third upload (upload_id=3)
  (3, 'Random_Presentation.pdf',  '/files/Random_Presentation.pdf',  'application/pdf', FALSE),

  -- For the fourth upload (upload_id=4)
  (4, 'Company_Deck_v2.pdf',      '/files/Company_Deck_v2.pdf',     'application/pdf', FALSE),
  (4, 'Some_Excel_Data.xlsx',     '/files/Some_Excel_Data.xlsx',    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', FALSE);

-- 4) Insert extractions
-- We'll pretend we've already extracted text from some files. 
-- That means "is_extracted" would be TRUE for them if we want to reflect that. 
-- Let’s do that in the insert for extractions directly:

-- For demonstration, let's insert extracted text for file_id = 1, 2, 3 (the PDFs or XLS from above).
-- This means you’d have run your pipeline on these files, got the text, and are saving it here.
-- We'll also set them to is_extracted=TRUE in the files table.
UPDATE files SET is_extracted = TRUE WHERE file_id IN (1, 2, 3);

INSERT INTO extractions (file_id, extracted_text, summarized_text)
VALUES
  (1, 
   'This is random extracted text from Fund_ABC_Financials.pdf. It mentions Fund ABC and total assets of $120M <think>some hidden reasoning about their capital structure</think>.', 
   'Fund ABC has $120M in assets.'),
  (2, 
   'Extracted text from Q1_Statements.xls, which lists total revenue of $50M. <think>some hidden reasoning about revenue breakdown</think>', 
   'Revenue stands at $50M.'),
  (3,
   'Overview about the company. They are interested in a partial exit. <think>this is hidden reasoning</think>',
   'Company is exploring a partial exit.');


-- 5) Insert named_entities (optional)
-- Let’s say we recognized 2 entities in file 1 and 1 entity in file 2.
INSERT INTO named_entities (file_id, entity_type, entity_name, confidence)
VALUES
  (1, 'FUND', 'Fund ABC', 0.95),
  (1, 'METRIC', '$120M', 0.90),
  (2, 'METRIC', '$50M', 0.88);

----------------------------------------

